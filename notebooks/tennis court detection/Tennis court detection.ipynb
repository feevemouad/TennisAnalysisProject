{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BfeTBrJoGmPo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_data(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    image_paths = []\n",
        "    keypoints = []\n",
        "    for item in data:\n",
        "        image_paths.append(\"../data/images/\" + str(item['id']) + '.png')  # Assuming JPG format\n",
        "        keypoints.append(item['kps'])  # List of 28 keypoints (x, y)\n",
        "\n",
        "    return image_paths, keypoints\n",
        "\n",
        "train_image_paths, train_keypoints = load_data('../data/data_train.json')\n",
        "val_image_paths, val_keypoints = load_data('../data/data_val.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O6IF3_lVidnL"
      },
      "outputs": [],
      "source": [
        "def parse_function(filename, keypoints):\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_image(image, channels=3)  # Decode the image\n",
        "    # image = tf.image.convert_image_dtype(image, tf.float32)  # Convert to float32\n",
        "\n",
        "    # Ensure the image tensor has a known shape\n",
        "    image = tf.ensure_shape(image, [None, None, 3])\n",
        "\n",
        "    # Resize the image\n",
        "    image = tf.image.resize(image, (224, 224))  # Resize image\n",
        "\n",
        "    keypoints  = tf.reshape(keypoints, [-1, 2])\n",
        "    keypoints *= tf.constant([[224/1280, 224/720]])\n",
        "    keypoints  = tf.reshape(keypoints, [-1])\n",
        "\n",
        "    return image, keypoints\n",
        "\n",
        "\n",
        "def create_dataset(image_paths, keypoints):\n",
        "    image_paths = tf.constant(image_paths)\n",
        "    keypoints = tf.constant(keypoints, dtype=tf.float32)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, keypoints))\n",
        "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = create_dataset(train_image_paths, train_keypoints)\n",
        "val_dataset = create_dataset(val_image_paths, val_keypoints)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-SevKOPAmHBC"
      },
      "outputs": [],
      "source": [
        "def prepare_for_training(ds, batch_size=128, shuffle_buffer_size=10000):  \n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_dataset = prepare_for_training(train_dataset)\n",
        "val_dataset = prepare_for_training(val_dataset, batch_size=16)  # Smaller batch for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W291FDMHwVMU",
        "outputId": "4425eb45-23e0-4b6a-bdc8-1f228974ab4a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV3Small\n",
        "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def create_model():\n",
        "    ## Define input layer\n",
        "    inputs = Input(shape=(224, 224, 3))\n",
        "    preprocessed_input = preprocess_input(inputs)  # Preprocess input images\n",
        "    \n",
        "    ## Load pre-trained model “Resnet50” without the final(top) layer\n",
        "    base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_tensor=preprocessed_input)\n",
        "    base_model.trainable = False\n",
        "    output = base_model.output\n",
        "\n",
        "    ## Condense feature maps from the output\n",
        "    output = Flatten()(output)\n",
        "\n",
        "\n",
        "    # Final layer has 28 output neurons\n",
        "    final_output = Dense(28, activation='relu')(output)  \n",
        "\n",
        "    ## Create our own network/model\n",
        "    model = Model(inputs=inputs, outputs=final_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_model()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(.0001), loss='mae')  # Using mean squared error loss for regression task\n",
        "\n",
        "earlystopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "# Define model checkpoint callback based on validation loss\n",
        "checkpoint_path = \"model_checkpoint.model.keras\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "415/415 [==============================] - 46s 52ms/step - loss: 17.6636 - val_loss: 5.0404\n",
            "Epoch 2/20\n",
            "415/415 [==============================] - 31s 43ms/step - loss: 4.1704 - val_loss: 3.7252\n",
            "Epoch 3/20\n",
            "415/415 [==============================] - 30s 42ms/step - loss: 3.2075 - val_loss: 3.1871\n",
            "Epoch 4/20\n",
            "415/415 [==============================] - 30s 43ms/step - loss: 2.7094 - val_loss: 2.9428\n",
            "Epoch 5/20\n",
            "415/415 [==============================] - 30s 43ms/step - loss: 2.3695 - val_loss: 2.6473\n",
            "Epoch 6/20\n",
            "415/415 [==============================] - 30s 43ms/step - loss: 2.1351 - val_loss: 2.5327\n",
            "Epoch 7/20\n",
            "415/415 [==============================] - 30s 43ms/step - loss: 1.9530 - val_loss: 2.4217\n",
            "Epoch 8/20\n",
            "415/415 [==============================] - 30s 43ms/step - loss: 1.7952 - val_loss: 2.3573\n",
            "Epoch 9/20\n",
            "415/415 [==============================] - 30s 43ms/step - loss: 1.6819 - val_loss: 2.2322\n",
            "Epoch 10/20\n",
            "415/415 [==============================] - 31s 43ms/step - loss: 1.5620 - val_loss: 2.1828\n",
            "Epoch 11/20\n",
            "415/415 [==============================] - 30s 42ms/step - loss: 1.4789 - val_loss: 2.1234\n",
            "Epoch 12/20\n",
            "415/415 [==============================] - 30s 43ms/step - loss: 1.3962 - val_loss: 2.1027\n",
            "Epoch 13/20\n",
            "415/415 [==============================] - 31s 44ms/step - loss: 1.3373 - val_loss: 2.0458\n",
            "Epoch 14/20\n",
            "415/415 [==============================] - 30s 43ms/step - loss: 1.2748 - val_loss: 2.0148\n",
            "Epoch 15/20\n",
            "415/415 [==============================] - 30s 42ms/step - loss: 1.2208 - val_loss: 2.0069\n",
            "Epoch 16/20\n",
            "415/415 [==============================] - 30s 42ms/step - loss: 1.1694 - val_loss: 1.9647\n",
            "Epoch 17/20\n",
            "415/415 [==============================] - 31s 45ms/step - loss: 1.1240 - val_loss: 1.9481\n",
            "Epoch 18/20\n",
            "415/415 [==============================] - 30s 42ms/step - loss: 1.0871 - val_loss: 1.9271\n",
            "Epoch 19/20\n",
            "415/415 [==============================] - 29s 42ms/step - loss: 1.0458 - val_loss: 1.9084\n",
            "Epoch 20/20\n",
            "415/415 [==============================] - 30s 43ms/step - loss: 1.0002 - val_loss: 1.8998\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=60, callbacks=[earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "52/52 [==============================] - 25s 219ms/step - loss: 0.4122 - val_loss: 1.6961\n",
            "Epoch 2/60\n",
            "52/52 [==============================] - 23s 213ms/step - loss: 0.3144 - val_loss: 1.6964\n",
            "Epoch 3/60\n",
            "52/52 [==============================] - 23s 215ms/step - loss: 0.3075 - val_loss: 1.6958\n",
            "Epoch 4/60\n",
            "52/52 [==============================] - 23s 218ms/step - loss: 0.3047 - val_loss: 1.6985\n",
            "Epoch 5/60\n",
            "52/52 [==============================] - 23s 214ms/step - loss: 0.3034 - val_loss: 1.6950\n",
            "Epoch 6/60\n",
            "52/52 [==============================] - 23s 209ms/step - loss: 0.2975 - val_loss: 1.6971\n",
            "Epoch 7/60\n",
            "52/52 [==============================] - 23s 218ms/step - loss: 0.2992 - val_loss: 1.7025\n",
            "Epoch 8/60\n",
            "52/52 [==============================] - 23s 219ms/step - loss: 0.2973 - val_loss: 1.6966\n",
            "Epoch 9/60\n",
            "52/52 [==============================] - 23s 214ms/step - loss: 0.2916 - val_loss: 1.6976\n",
            "Epoch 10/60\n",
            "52/52 [==============================] - 23s 213ms/step - loss: 0.2941 - val_loss: 1.6975\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(.0001), loss='mae')  # Using mean squared error loss for regression task\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=60, callbacks=[earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('models/keypoints_1_6950"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
